# KVCache Chatbot

A modern multi-turn conversation chatbot with document upload support and KV cache optimization, featuring a clean separation between frontend (Gradio) and backend (FastAPI).

## âœ¨ Key Features

- ğŸ’¬ **Multi-turn Conversations**: Maintains conversation history for context-aware responses
- ğŸ“„ **Document Management**: Upload and select documents independently from chat sessions with PDF support
- âš¡ **Streaming Responses**: Real-time streaming for better user experience
- ğŸš€ **KV Cache Integration**: Optimized document processing with pre-caching for local models
- ğŸ”§ **Flexible Model Management**: Support for both local and remote models with dynamic switching
- ğŸ§  **Reasoning Models Support**: Enhanced configuration for reasoning models (e.g., GPT-OSS-20B) with extended timeouts
- ğŸ¨ **Configurable Prompts**: Customizable prompt templates for fine-tuned control over conversation and RAG contexts
- ğŸ”Œ **Clean Architecture**: RESTful API with frontend-backend separation

## ğŸš€ Quick Start

### Prerequisites
- **Python 3.9+** (recommended: Python 3.10+)
- **uv** package manager (recommended) or pip
- **Git** (for cloning the repository)

### Installation & Setup

**Option A: Using Make (Recommended)**
```bash
git clone <repository-url>
cd kvcache_chatbot
make start
```

**Option B: Using npm**
```bash
git clone <repository-url>
cd kvcache_chatbot
npm start
```

**Option C: Manual Setup (Recommended for First-Time Setup)**

#### âš™ï¸ 1. Setup Configuration
```bash
cd src/api
cp env.example.yaml env.yaml
```
Overwrite or modify `env.yaml` with this [configuration](https://hackmd.io/@kenyo3023/SJJ11_dk-e)

---

#### ğŸ› ï¸ 2. Start Service

##### ğŸ‘ï¸ For better viewing experience, you can execute the frontend and backend separately

**Install Backend**
```bash
pip install -r src/api/requirements.txt
```

**Execute Backend**
```bash
cd src/api && python main.py --port 3023
# Use 'cd src/api && python main.py --help' to view the available options
```
Then view the **backend API docs** in browser through `http://localhost:3023/docs`

**Install Frontend**
```bash
pip install -r src/web/requirements.txt
```

**Execute Frontend**
```bash
cd src/web && python app.py --backend-port 3023
# Use 'cd src/web && python app.py --help' to view the available options
```
Then view the **frontend website** in browser through `http://localhost:7860`

> **Important**: The backend `--port` and frontend `--backend-port` must be aligned

##### ğŸ” Monitor Model Deployment Progress

**Monitor Progress** (Windows)
```powershell
netstat -ano | findstr :13141  # This is the default model deployment port
# Command template: netstat -ano | findstr :{MODEL_DEPLOY_PORT:-13141}
```

**Monitor Progress** (Linux/macOS)
```bash
lsof -i :13141  # This is the default model deployment port
# Command template: lsof -i :{MODEL_DEPLOY_PORT:-13141}
```

### Access the Application
- **Frontend**: http://localhost:7860
- **Backend API**: http://localhost:3023 (default, customizable via `--port`)
- **API Documentation**: http://localhost:3023/docs

## ğŸ“š Documentation

### ğŸ› ï¸ Installation Guide
For detailed installation instructions, dependency management, and configuration:
**[ğŸ“– GETTING_STARTED.md](docs/GETTING_STARTED.md)**

### ğŸ¯ Usage Guide
For comprehensive usage instructions, workflows, and troubleshooting:
**[ğŸ“– QUICK_START.md](docs/QUICK_START.md)**

### ğŸ—ï¸ Architecture Overview
For system architecture, data flow, and technical details:
**[ğŸ“– ARCHITECTURE.md](docs/ARCHITECTURE.md)**

## ğŸ® Basic Usage

1. **Configure Models**: Set up your models in `src/api/env.yaml` (local and/or remote)
2. **Select Model**: Choose from the model dropdown (shows type indicators)
3. **Start Local Models**: Click "Start with Reset" or "Start without Reset" for local models (remote models are ready immediately)
4. **Upload Documents**: Use the left sidebar to upload `.pdf` files
5. **Select Document**: Choose from dropdown to use document context
6. **Cache Documents**: Use "Cache" button for faster document processing (KV cache for local models)
7. **Chat**: Type messages and get responses with document context and streaming

## ğŸ”§ Configuration

### Configuration File Setup

Create `env.yaml` in `src/api/` directory (copy from `env.example.yaml`):

```bash
cd src/api
cp env.example.yaml env.yaml
```

### YAML Configuration Structure

The system uses YAML format for flexible model and server configuration:

```yaml
server:
    exe_path: C:\path\to\llama-server.exe
    log_path: C:\path\to\logs\maestro_llama.log
    cache_dir: R:\

models:
    # Local models (self-hosted)
    local_models:
      - model_name_or_path: /path/to/model.gguf
        serving_name: my-local-model
        tokenizer: meta-llama/Llama-3.1-8B-Instruct
        base_url: http://localhost:13141/v1
        api_key: not-needed
        completion_params:
            temperature: 0.7
            max_tokens: 2000
        # Optional: Serving parameters for model server
        serving_params:
            ctx_size: 16384
            n_gpu_layers: 100
            timeout: 900  # Extended timeout for reasoning models

      # GPT-OSS-20B Reasoning Model - Recommended Configuration
      - model_name_or_path: /path/to/gpt-oss-20b-GGUF.gguf
        serving_name: ggml-org/gpt-oss-20b-GGUF
        tokenizer: openai/gpt-oss-20b
        base_url: http://localhost:13141/v1
        api_key: not-needed
        completion_params:
            temperature: 0.0
            max_tokens: 20000
            # Recommended completion parameters for GPT-OSS-20B reasoning model
            repeat_penalty: 1.1
            repeat_last_n: 64
            chat_template_kwargs:
                reasoning_effort: low
        serving_params: # Llamacpp serving args
            log_file: /path/to/logging.log
            # Recommended serving parameters for GPT-OSS-20B reasoning model
            # More details can visit https://llama-cpp-python.readthedocs.io/en/latest/api-reference/
            ctx_size: 131072
            reasoning_format: deepseek
            jinja: True
            swa_full: True

  # Remote models (APIs, other machines)
  remote_models:
        # OpenAI
      - provider: openai
        model: gpt-4o-mini
        serving_name: gpt-4o-mini
        base_url: https://api.openai.com/v1
        api_key: YOUR_OPENAI_API_KEY
        completion_params:
            temperature: 0.7
            max_tokens: 4000

        # Google Gemini (OpenAI-compatible)
      - provider: openai
        model: gemini-2.0-flash
        serving_name: gemini-2.0-flash
        base_url: https://generativelanguage.googleapis.com/v1beta/openai/
        api_key: YOUR_GOOGLE_GEMINI_API_KEY
        completion_params:
            temperature: 0.5
            max_tokens: 8000

# Optional: Custom prompt templates
prompts:
  system_prompt_template: |
    ### Task:
    Respond to the user query using the provided context.

    ### Guidelines:
    - If you don't know the answer, clearly state that.
    - Respond in the same language as the user's query.

    ### Output:
    Provide a clear and direct response.

    <context>
    {doc_context}
    </context>

  user_prompt_template: |
    <user_query>
    {user_query}
    </user_query>

# Optional: Document processing settings
documents:
  upload_dir: uploads
  max_file_size: 52428800  # 50MB
  allowed_extensions: [".pdf"]
  chunk_size: 10000
  chunk_overlap: 200
```

**Note**:
- For detailed configuration examples, see [env.example.yaml](src/api/env.example.yaml)
- Remote models are ready immediately, local models require starting the server
- Extended timeout (900s) recommended for reasoning models like GPT-OSS-20B

## ğŸ› ï¸ Development

### Project Structure
```
src/
â”œâ”€â”€ api/              # FastAPI backend
â”‚   â”œâ”€â”€ main.py       # Main application
â”‚   â”œâ”€â”€ config.py     # Configuration
â”‚   â”œâ”€â”€ models.py     # Data models
â”‚   â”œâ”€â”€ routers/      # API routes
â”‚   â”‚   â”œâ”€â”€ document.py    # Document management
â”‚   â”‚   â”œâ”€â”€ session.py     # Chat sessions
â”‚   â”‚   â”œâ”€â”€ model.py       # Model management
â”‚   â”‚   â””â”€â”€ logs.py        # Log management
â”‚   â””â”€â”€ services/     # Business logic
â”‚       â”œâ”€â”€ document_manager.py  # Document storage
â”‚       â”œâ”€â”€ session_service.py   # Session management
â”‚       â”œâ”€â”€ llm_service.py       # LLM integration
â”‚       â””â”€â”€ model.py             # Model server management
â””â”€â”€ web/              # Gradio frontend
    â””â”€â”€ app.py        # Gradio interface
```

### Available Commands
```bash
make start          # Start both frontend and backend
make stop           # Stop all services
make install        # Install dependencies
make test           # Health check
make clean          # Clean logs and uploads
```

## ğŸ”— API Endpoints

### Document Management
- `POST /api/v1/documents/upload` - Upload document
- `POST /api/v1/documents/upload_and_cache` - Upload and cache document
- `GET /api/v1/documents/list` - List all documents
- `DELETE /api/v1/documents/{doc_id}` - Delete document
- `POST /api/v1/documents/cache/{doc_id}` - Cache existing document

### Chat Sessions
- `GET /api/v1/session/{session_id}` - Get session info
- `POST /api/v1/session/{session_id}/messages/stream` - Send message (streaming)
- `GET /api/v1/session/{session_id}/messages` - Get chat history
- `DELETE /api/v1/session/{session_id}` - Delete session

### Model Management
- `GET /api/v1/model/list` - List all models (local and remote) with type information
- `POST /api/v1/model/switch` - Switch to a different model
- `POST /api/v1/model/up/reset` - Start local model with reset
- `POST /api/v1/model/up/without_reset` - Start local model without reset
- `POST /api/v1/model/down` - Stop local model
- `GET /api/v1/model/status` - Get current model status

## ğŸš¨ Troubleshooting

### Common Issues
1. **Port already in use**: `make stop` or `npm run stop`
2. **Dependencies not installed**: `make install` or `npm run install-deps`
3. **Permission errors on Windows**: Run as Administrator or use WSL

### Health Check
```bash
make test
# or
npm test
```

## ğŸ—ï¸ Technology Stack

**Backend:**
- FastAPI - Modern Python web framework
- Pydantic - Data validation and serialization
- Uvicorn - ASGI server
- httpx - Async HTTP client for full API parameter support
- PyYAML - YAML configuration parsing

**Frontend:**
- Gradio - ML/AI web interface framework
- httpx - HTTP client for API communication

**Development:**
- uv - Fast Python package manager
- Make - Cross-platform build automation
- npm - Node.js package management

## ğŸ“„ License

MIT

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

**Need help?** Check out our detailed documentation:
- **[GETTING_STARTED.md](docs/GETTING_STARTED.md)** - Installation and setup
- **[QUICK_START.md](docs/QUICK_START.md)** - Usage guide and workflows
- **[ARCHITECTURE.md](docs/ARCHITECTURE.md)** - System architecture overview
- **[LOCAL_REMOTE_MODELS.md](docs/LOCAL_REMOTE_MODELS.md)** - Local and remote models configuration
- **[DOCUMENT_AND_RAG.md](docs/DOCUMENT_AND_RAG.md)** - Document processing and RAG implementation
