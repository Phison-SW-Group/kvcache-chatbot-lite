server:
  exe_path: C:\Program Files\Phison\AgentBuilderClient\MaestroMcpServer\maestro\llama-server.exe
  log_path: C:\Program Files\Phison\AgentBuilderClient\logs\maestro_llama.log
  cache_dir: R:\

models:
  # Each model can have its own API endpoint and completion parameters
  - model_name_or_path: /path/to/your/model.gguf
    serving_name: my-model-name
    tokenizer: meta-llama/Llama-3.1-8B-Instruct

    # API settings for this model
    base_url: http://localhost:13141/v1
    api_key: not-needed

    # Completion parameters for this model
    completion_params:
      # Basic generation parameters
      temperature: 0.7
      max_tokens: 2000
      top_p: 1.0
      top_k: 40
      repeat_penalty: 1.1

      # Advanced parameters
      frequency_penalty: 0.0
      presence_penalty: 0.0
      min_p: 0.05
      tfs_z: 1.0
      typical_p: 1.0

      # Mirostat parameters
      mirostat: 0
      mirostat_tau: 5.0
      mirostat_eta: 0.1

      # Token handling
      repeat_last_n: 64
      penalize_newline: true
      add_bos_token: true
      ban_eos_token: false
      skip_special_tokens: true

      # Streaming and seed
      stream: false
      seed: -1

      # Stop sequences (optional)
      stop: null

      # Custom parameters - any additional llama-cpp-python parameters
      # These will be passed directly to the llama-cpp-python library
      # Reference: https://llama-cpp-python.readthedocs.io/en/latest/api-reference/
      # Example custom parameters:
      # n_gpu_layers: 10
      # n_ctx: 2048
      # n_batch: 512
      # n_threads: 4
      # verbose: true

  # Example: Multiple models with different endpoints
  # - model_name_or_path: /path/to/another/model.gguf
  #   serving_name: another-model
  #   tokenizer: meta-llama/Llama-3.1-8B-Instruct
  #   base_url: http://localhost:13142/v1
  #   api_key: different-api-key
  #   completion_params:
  #     temperature: 0.5
  #     max_tokens: 4000

  # Example: Minimal model configuration (only for tokenizer or reference)
  # - model_name_or_path: /path/to/third/model.gguf
  #   serving_name: tokenizer-only-model
  #   tokenizer: meta-llama/Llama-3.1-8B-Instruct
  #   # api_key defaults to "empty"
  #   # base_url defaults to None
  #   # completion_params use defaults

# Document processing settings (optional)
# Uncomment to customize document processing behavior
# documents:
#   upload_dir: uploads
#   max_file_size: 52428800  # 50MB
#   allowed_extensions: [".pdf"]
#   chunk_size: 5000
#   chunk_overlap: 200
#   # Optional: Specify which model's tokenizer to use for token counting
#   # If not set, will automatically use the first model with a tokenizer configured
#   # tokenizer_model: my-model-name
