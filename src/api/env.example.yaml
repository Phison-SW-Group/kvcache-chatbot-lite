server:
  exe_path:
  log_path:
  cache_dir:

models:
  - model_name_or_path:
    serving_name:
  - model_name_or_path:
    serving_name: null

completion_params:
  # API settings
  base_url: http://localhost:13141/v1
  api_key: not-needed

  # Basic generation parameters
  temperature: 0.0
  max_tokens: 20000
  top_p: 1.0
  top_k: 40
  repeat_penalty: 1.1

  # Advanced parameters
  frequency_penalty: 0.0
  presence_penalty: 0.0
  min_p: 0.05
  tfs_z: 1.0
  typical_p: 1.0

  # Mirostat parameters
  mirostat: 0
  mirostat_tau: 5.0
  mirostat_eta: 0.1

  # Token handling
  repeat_last_n: 64
  penalize_newline: true
  add_bos_token: true
  ban_eos_token: false
  skip_special_tokens: true

  # Streaming and seed
  stream: false
  seed: -1

  # Stop sequences (optional)
  stop: null

  # Custom parameters - any additional llama-cpp-python parameters
  # These will be passed directly to the llama-cpp-python library
  # Reference: https://llama-cpp-python.readthedocs.io/en/latest/api-reference/
  # Example custom parameters:
  # n_gpu_layers: 10
  # n_ctx: 2048
  # n_batch: 512
  # n_threads: 4
  # verbose: true
