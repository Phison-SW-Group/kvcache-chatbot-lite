server:
  exe_path: /absolute/path/to/your/llama-server.exe
  log_path: /absolute/path/to/your/server-logging.log
  cache_dir: R:\

models:
  # New structure: local_models (self-hosted) and remote_models (APIs/other machines)
  local_models:

    - model_name_or_path: /absolute/path/to/your/model.gguf
      serving_name: my-local-model
      tokenizer: meta-llama/Llama-3.1-8B-Instruct
      # Local llama-server (OpenAI-compatible)
      base_url: http://localhost:13141/v1
      api_key: not-needed
      completion_params:
        # Basic generation parameters
        temperature: 0.7
        max_tokens: 2000

      # GPT-OSS-20B Reasoning Model - Recommended Configuration
    - model_name_or_path: /absolute/path/to/gpt-oss-20b-GGUF.gguf
      serving_name: ggml-org/gpt-oss-20b-GGUF
      tokenizer: openai/gpt-oss-20b
      base_url: http://localhost:13141/v1
      api_key: not-needed
      completion_params:
          temperature: 0.8
          max_tokens: 20000
          # Recommended completion parameters for GPT-OSS-20B reasoning model
          repeat_penalty: 1.1
          repeat_last_n: 64
          chat_template_kwargs:
              reasoning_effort: low
      serving_params: # Llamacpp serving args
          log_file: /path/to/logging.log
          # Recommended serving parameters for GPT-OSS-20B reasoning model
          # More details can visit https://llama-cpp-python.readthedocs.io/en/latest/api-reference/
          ctx_size: 131072
          reasoning_format: deepseek
          jinja: True
          swa_full: True

  remote_models:
    # Example: OpenAI
    - provider: openai
      model: gpt-4o-mini
      serving_name: gpt-4o-mini
      base_url: https://api.openai.com/v1
      api_key: YOUR_OPENAI_API_KEY
      completion_params:
        temperature: 0.7
        max_tokens: 4000

    # Example: Google Gemini (OpenAI-compatible endpoint)
    - provider: openai
      model: gemini-2.0-flash
      serving_name: gemini-2.0-flash
      # Note: Use the OpenAI-compatible path provided by Google
      base_url: https://generativelanguage.googleapis.com/v1beta/openai/
      api_key: YOUR_GOOGLE_GEMINI_API_KEY
      completion_params:
        temperature: 0.5
        max_tokens: 8000

# Document processing settings (optional)
# Uncomment to customize document processing behavior
# documents:
#   upload_dir: uploads
#   max_file_size: 52428800  # 50MB
#   allowed_extensions: [".pdf"]
#   chunk_size: 5000
#   chunk_overlap: 200
#   # Optional: Specify which model's tokenizer to use for token counting
#   # If not set, will automatically use the first model with a tokenizer configured
#   # tokenizer_model: my-local-model

prompts:
  system_prompt_template: |
    ### Task:
    Respond to the user query using the provided context, incorporating inline citations in the format [id] **only when the <source> tag includes an explicit id attribute** (e.g., <source id="1">).

    ### Guidelines:
    - If you don't know the answer, clearly state that.
    - If uncertain, ask the user for clarification.
    - Respond in the same language as the user's query.
    - If the context is unreadable or of poor quality, inform the user and provide the best possible answer.
    - If the answer isn't present in the context but you possess the knowledge, explain this to the user and provide the answer using your own understanding.
    - **Only include inline citations using [id] (e.g., [1], [2]) when the <source> tag includes an id attribute.**
    - Do not cite if the <source> tag does not contain an id attribute.
    - Do not use XML tags in your response.
    - Ensure citations are concise and directly related to the information provided.

    ### Example of Citation:
    If the user asks about a specific topic and the information is found in a source with a provided id attribute, the response should include the citation like in the following example:
    * "According to the study, the proposed method increases efficiency by 20% [1]."

    ### Output:
    Provide a clear and direct response to the user's query, including inline citations in the format [id] only when the <source> tag with id attribute is present in the context.

    <context>
    {doc_context}
    </context>