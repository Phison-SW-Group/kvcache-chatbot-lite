server:
  exe_path: C:\Program Files\Phison\AgentBuilderClient\MaestroMcpServer\maestro\llama-server.exe
  log_path: C:\Program Files\Phison\AgentBuilderClient\logs\maestro_llama.log
  cache_dir: R:\

models:
  # New structure: local_models (self-hosted) and remote_models (APIs/other machines)
  local_models:
    - model_name_or_path: /absolute/path/to/your/model.gguf
      serving_name: my-local-model
      tokenizer: meta-llama/Llama-3.1-8B-Instruct
      # Local llama-server (OpenAI-compatible)
      base_url: http://localhost:13141/v1
      api_key: not-needed
      completion_params:
        # Basic generation parameters
        temperature: 0.7
        max_tokens: 2000

  remote_models:
    # Example: OpenAI
    - provider: openai
      model: gpt-4o-mini
      serving_name: gpt-4o-mini
      base_url: https://api.openai.com/v1
      api_key: YOUR_OPENAI_API_KEY
      completion_params:
        temperature: 0.7
        max_tokens: 4000

    # Example: Google Gemini (OpenAI-compatible endpoint)
    - provider: openai
      model: gemini-2.0-flash
      serving_name: gemini-2.0-flash
      # Note: Use the OpenAI-compatible path provided by Google
      base_url: https://generativelanguage.googleapis.com/v1beta/openai/
      api_key: YOUR_GOOGLE_GEMINI_API_KEY
      completion_params:
        temperature: 0.5
        max_tokens: 8000

# Document processing settings (optional)
# Uncomment to customize document processing behavior
# documents:
#   upload_dir: uploads
#   max_file_size: 52428800  # 50MB
#   allowed_extensions: [".pdf"]
#   chunk_size: 5000
#   chunk_overlap: 200
#   # Optional: Specify which model's tokenizer to use for token counting
#   # If not set, will automatically use the first model with a tokenizer configured
#   # tokenizer_model: my-local-model
